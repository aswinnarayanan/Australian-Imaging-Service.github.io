










































[{"body":"How to Use XNAT https://wiki.xnat.org/documentation/how-to-use-xnat\nXNAT Desktop Client (DXM) https://wiki.xnat.org/xnat-tools/xnat-desktop-client-dxm\nhttps://wiki.xnat.org/xnat-tools/xnat-desktop-client-dxm/uploading-image-sessions\n","categories":"","description":"","excerpt":"How to Use XNAT https://wiki.xnat.org/documentation/how-to-use-xnat …","ref":"/docs/getting-started/","tags":"","title":"AIS User guides"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/facility-guides/","tags":"","title":"Facility guides"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/instruments/","tags":"","title":"Instruments"},{"body":"In each case, we recommend raising issue tickets within the repository in question.\nXNAT-build This repository builds an XNAT 1.7.6 instance in docker or linux containers (lxd). This is suitable for local deployments.\nCode: xnat-build\nDocumentation: https://australian-imaging-service.github.io//xnat-build\nCharts This is an alternative deployment process utilising Helm and Kubernetes. This is suitable for cloud deployments.\nCode: charts\nDocumentation: https://australian-imaging-service.github.io/docs/charts/\nXNATUtils Xnat-utils is a collection of scripts for conveniently up/downloading and listing data on/from XNAT based on the XnatPy package.\nCode: xnatutils\nDocumentation: https://australian-imaging-service.github.io//xnatutils\ns3fs-build A side container to mount Amazon S3 storage in a docker container’s filesystem.\nCode: s3fs-build\nDocumentation: https://australian-imaging-service.github.io//s3fs-build\nqc-pipelines A collection of QA pipelines for assessing quality biomedical images\nCode: qc-pipelines\nDocumentation: https://australian-imaging-service.github.io//qc-pipelines\nCTP-build Clinical Trials Processor (CTP) is used for processing images from clinical trials. XNAT utilises CTP for anonymisation.\nCode: ctp-build\nDocumentation: https://australian-imaging-service.github.io//CTP-build\nxnat-openid-auth-plugin Provides AAF authentication in particular.\nCode: xnat-openid-auth-plugin\nDocumentation: https://australian-imaging-service.github.io//xnat-openid-auth-plugin\n","categories":"","description":"","excerpt":"In each case, we recommend raising issue tickets within the repository …","ref":"/docs/repositories/","tags":"","title":"Top level overview of the AIS repositories"},{"body":"Applying for AAF Integration ClientId and Secret AAF have several services they offer which authenticate users, for example, Rapid Connect. We are interested in the AAF OIDC RP service. Please contact AAF Support via email at support@aaf.net.au to apply for a ClientId and Secret.\nThey will ask you these questions:\n The service’s redirect URL - a redirect URL based on an actual URL rather than IP address and must use HTTPS. A descriptive name for the service. The organisation name, which must be an AAF subscriber, of the service. Indicate the service’s purpose - development/testing/production-ready. Your Keybase account id to share the credentials securely.  For 1. This is extremely important and based on two options in the openid-provider.properties file:\n siteUrl preEstablishedRedirUri  We will use this example below (this is the correct syntax):\n openid-provider.properties siteUrl=https://xnat.example.com  preEstablishedRedirUri=/openid-login  In this case, the answer to 1 should be https://xnat.example.com/openid-login Submitting https://xnat.example.com will lead to a non functional AAF setup.\nCan be anything – preferably descriptive. Exactly what it says. Mostly the university name depending on organisation This is important as it will dictate the AAF Servers your service will authenticate against.  If it is a testing or development environment, you will use the following details:\nopenid.aaf.accessTokenUri=https://central.test.aaf.edu.au/providers/op/token  openid.aaf.userAuthUri=https://central.test.aaf.edu.au/providers/op/authorize For production environments (notice no test in the URLs):\nopenid.aaf.accessTokenUri=https://central.aaf.edu.au/providers/op/token  openid.aaf.userAuthUri=https://central.aaf.edu.au/providers/op/authorize For 5. Just go to https://keybase.io/ and create an account to provide to AAF support so you can receive the ClientId and ClientSecret securely.\nInstalling the AAF Plugin in a working XNAT environment There have been long standing issues with the QCIF plugin that have been resolved by the AIS Deployment team – namely unable to access any projects – see image below.\nThis issue occurred regardless of project access permissions. You would receive this error message trying to access your own project!\nAIS Deployment team created a forked version of the plugin which fixes this issue. You can view it here:\nhttps://github.com/Australian-Imaging-Service/xnat-openid-auth-plugin\nTo deploy to XNAT, navigate to the XNAT home/ plugins folder on your XNAT Application Server – normally /data/xnat/home/plugins and then download. Assuming Linux:\nwget https://github.com/Australian-Imaging-Service/xnat-openid-auth-plugin/releases/download/1.0.2/xnat-openid-auth-plugin-all-1.0.2.jar  Please note this was the latest version at the time of writing this document. Please check here to see if there have been updated versions:\nhttps://github.com/Australian-Imaging-Service/xnat-openid-auth-plugin/releases\n You now have xnat-openid-auth-plugin-all-1.0.2.jar in /data/xnat/home/plugins.\nYou now need the configuration file which will be (assuming previous location for XNAT Home directory):\n/data/xnat/home/config/auth/openid-provider.properties\nYou will need to create this file.\nReview this sample file and tailor to your needs:\nhttps://github.com/Australian-Imaging-Service/xnat-openid-auth-plugin/blob/master/src/main/resources/openid-provider-sample-AAF.properties\nI will provide an example filled out properties file with some caveats below.\nWarning All of the keys are case sensitive, incorrectly capitalised entries will result in non-working AAF integration!  These need to be left as is\nauth.method=openid  type=openid  provider.id=openid  visible=true  Set these values to false if you want an Admin to enable and verify the account before users are allowed to login - recommended\nauto.enabled=false  auto.verified=false Name displayed in the UI – not particularly important\nname=OpenID Authentication Provider Toggle username \u0026 password login visibility\ndisableUsernamePasswordLogin=false List of providers that appear on the login page, see options below. In our case we only need aaf but you can have any openid enabled provider\nenabled=aaf Site URL - the main domain, needed to build the pre-established URL below. See notes at top of document\nsiteUrl=https://xnat.example.com  preEstablishedRedirUri=/openid-login AAF ClientID and Secret – CASE SENSITIVE - openid.aaf.clientID for example would mean AAF plugin will not function These are fake details but an example – no “ (quotation marks) required.\nopenid.aaf.clientId=123jsdjd  openid.aaf.clientSecret=chahdkdfdhffkhf The providers are covered at the top of the document\nopenid.aaf.accessTokenUri=https://central.test.aaf.edu.au/providers/op/token  openid.aaf.userAuthUri=https://central.test.aaf.edu.au/providers/op/authorize You can find more details on the remaining values here:\nhttps://github.com/Australian-Imaging-Service/xnat-openid-auth-plugin\nopenid.aaf.scopes=openid,profile,email If the below is wrong the AAF logo will not appear on the login page and you won’t be able to login\nopenid.aaf.link=\u003cp\u003eTo sign-in using your AAF credentials, please click on the button below.\u003c/p\u003e\u003cp\u003e\u003ca href=\"/openid-login?providerId=aaf\"\u003e\u003cimg src=\"/images/aaf_service_223x54.png\" /\u003e\u003c/a\u003e\u003c/p\u003e Flag that sets if we should be checking email domains\nopenid.aaf.shouldFilterEmailDomains=false Domains below are allowed to login, only checked when shouldFilterEmailDomains is true\nopenid.aaf.allowedEmailDomains=example.com  Flag to force the user creation process, normally this should be set to true\nopenid.aaf.forceUserCreate=true Flag to set the enabled property of new users, set to false to allow admins to manually enable users before allowing logins, set to true to allow access right away\nopenid.aaf.userAutoEnabled=false Flag to set the verified property of new users – use in conjunction with auto.verified\nopenid.aaf.userAutoVerified=false Property names to use when creating users\nopenid.aaf.emailProperty=email  openid.aaf.givenNameProperty=name  openid.aaf.familyNameProperty=deliberately_unknown_property  If you create your openid-provider.properties file with the above information, tailored to your environment, along with the plugin:\n/data/xnat/home/plugins/xnat-openid-auth-plugin-all-1.0.2.jar\nYou should only need to restart Tomcat to enable login. This assumes you have a valid AAF organisation login.\nUsing AAF with the AIS Kubernetes Chart Deployment The AIS Charts Helm template has all you need to setup a completely functional XNAT implementation in minutes, part of this is AAF integration. Prerequisites: •\tA functional HTTPS URL with valid SSL certificate for your Kubernetes cluster. See the top of this document for details to provide to AAF.\n•\tA ClientId and Secret provided by AAF.\n•\tA Load Balancer or way to connect externally to your Kubernetes using the functional URL with SSL certificate.\nBefore you deploy the Helm template, clone it via git here:\ngit clone https://github.com/Australian-Imaging-Service/charts.git\nthen edit the following file:\ncharts/releases/xnat/charts/xnat-web/values.yaml\nAnd update the following entries underneath openid:\nNB\u003e These entries DO require being placed within “”  preEstablishedRedirUri:\"/openid-login\"siteUrl:\"\"#List of providers that appear on the login pageproviders:aaf:accessTokenUri:https://central.aaf.edu.au/providers/op/token#accessTokenUri: https://central.test.aaf.edu.au/providers/op/tokenuserAuthUri:https://central.aaf.edu.au/providers/op/authorize#userAuthUri: https://central.test.aaf.edu.au/providers/op/authorizeclientId:\"\"clientSecret:\"\"Comment out the Test or Production providers depending on which environment your XNAT will reside in. To use the example configuration from the previous configuration, the completed entries will look like this:\npreEstablishedRedirUri:\"/openid-login\"siteUrl:\"https://xnat.example.com\"#List of providers that appear on the login pageproviders:aaf:accessTokenUri:https://central.test.aaf.edu.au/providers/op/tokenuserAuthUri:https://central.test.aaf.edu.au/providers/op/authorizeclientId:\"123jsdjd\"clientSecret:\"chahdkdfdhffkhf\"You can now deploy your Helm template by following the README here: https://github.com/Australian-Imaging-Service/charts In order for this to work, you will need to point your domain name and SSL certificate to the Kubernetes xnat-web pod, which is outside of the scope of this document.\nTroubleshooting Most of the above documentation should remove the need for troubleshooting but a few things to bear in mind.\n  All of the openid-provider.properties file and the values.yaml file mentioned above for either existing XNAT deployments are CASE SENSITIVE. The entries must match exactly AAF won’t work.\n  If you get a 400 error message when redirecting from XNAT to AAF like so:\nhttps://central.test.aaf.edu.au/providers/op/authorize?client_id=\u0026redirect_uri=https://xnat.example.com/openid-login\u0026response_type=code\u0026scope=openid%20profile%20email\u0026state=IcoFrh\nThe ClientId entry is wrong. This happened before when the properties file had ClientId like this:\nopenid.aaf.clientID rather than:\nopenid.aaf.clientId You can see client_id section is empty. This wrongly capitalised entry results in the clientId not be passed to the URL to redirect and a 400 error message.\n  Check the log files. The most useful log file for error messages is the Tomcat localhost logfile. On RHEL based systems, this can be found here (example logfile):\n/var/log/tomcat7/localhost.2021-08-08.log You can also check the XNAT logfiles, mostly here (depending on where XNAT Home is on your system):\n/data/xnat/home/logs   ","categories":"","description":"","excerpt":"Applying for AAF Integration ClientId and Secret AAF have several …","ref":"/docs/charts/operations/aaf-integration/","tags":"","title":"Integrating AAF with AIS Kubernetes XNAT Deployment"},{"body":"Creating an Application Load Balancer to connect to the AIS Helm chart XNAT Implementation We will be following this AWS Guide:\nhttps://docs.aws.amazon.com/eks/latest/userguide/alb-ingress.html\nBefore we begin One thing that you need to know when we want to create new ALB from EKS is service spec type can only support LoadBalancer and NodePort. It won’t support ClusterIP.  The Charts Repo has the service defined as ClusterIP so some changes need to be made to make this work. We will get to that later after we have created the ALB and policies.\nIn this document we create a Cluster called xnat in ap-southeast-2. Please update these details for your environment.\nCreate an IAM OIDC provider and associate with cluster:\neksctl utils associate-iam-oidc-provider --region ap-southeast-2 --cluster xnat --approve Download the IAM Policy:\ncurl -o iam-policy.json https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/main/docs/install/iam_policy.json Create the IAM policy and take a note of the ARN:\naws iam create-policy --policy-name AWSLoadBalancerControllerIAMPolicy --policy-document file://iam-policy.json Create the service account using ARN from the previous command (substitute your ARN for the XXX):\neksctl create iamserviceaccount --cluster=xnat --namespace=kube-system --name=aws-load-balancer-controller --attach-policy-arn=arn:aws:iam::XXXXXXXXX:policy/AWSLoadBalancerControllerIAMPolicy --override-existing-serviceaccounts --approve Install TargetGroupBinding:\nkubectl apply -k \"github.com/aws/eks-charts/stable/aws-load-balancer-controller//crds?ref=master\" Download the EKS Helm Chart and update repo information:\nhelm repo add eks https://aws.github.io/eks-charts helm repo update Install the AWS Load Balancer Controller:\nhelm upgrade -i aws-load-balancer-controller eks/aws-load-balancer-controller --set clusterName=xnat --set serviceAccount.create=false --set serviceAccount.name=aws-load-balancer-controller -n kube-system Confirm it is installed:\nkubectl get deployment -n kube-system aws-load-balancer-controller You should see - READY 1/1 if it is installed properly\nIn order to apply this to the XNAT Charts Helm template update the charts/xnat/values.yaml file to remove the Nginx ingress parts and add the ALB ingress parts.\nAdded to values file:\nkubernetes.io/ingress.class:albalb.ingress.kubernetes.io/scheme:internet-facingalb.ingress.kubernetes.io/group.name:xnatalb.ingress.kubernetes.io/target-type:ip NB. Although you can specify ip or instance for the target-type, you need to specify ip or autoscaling won’t function correctly. This is because stickiness isn’t honoured for target-type instance so you have the known issue where XNAT database thinks you are logged in but instance / pod knows you are not and then it logs you out again.  For more ALB annotations / options, please see article at the bottom of the page.\nCommented out / removed:\nkubernetes.io/ingress.class:\"nginx\"kubernetes.io/tls-acme:\"true\"nginx.ingress.kubernetes.io/whitelist-source-range:\"130.95.0.0/16 127.0.0.0/8\"nginx.ingress.kubernetes.io/proxy-connect-timeout:\"150\"nginx.ingress.kubernetes.io/proxy-send-timeout:\"100\"nginx.ingress.kubernetes.io/proxy-read-timeout:\"100\"nginx.ingress.kubernetes.io/proxy-buffers-number:\"4\"nginx.ingress.kubernetes.io/proxy-buffer-size:\"32k\"As pointed out ClusterIP as service type does not work with ALB. So you will have to make some further changes to charts/xnat/charts/xnat-web/values.yaml:\nChange:\nservice:type:ClusterIPport:80to:\nservice:type:NodePortport:80In xnat/charts/xnat-web/templates/service.yaml remove the line:\nclusterIP:NoneThen create the Helm chart with the usual command (after building dependencies - just follow README.md). If you are updating an existing xnat installation it will fail so you will need to create a new application.\nhelm upgrade xnat . -nxnat It should now create a Target Group and Application Load Balancer in AWS EC2 Services. I had to make a further change to get this to work.\nOn the Target Group I had to change health check code from 200 to 302 to get a healthy instance because it redirects.\nYou can fix this by adding the following line to values file:\n# Specify Health Checksalb.ingress.kubernetes.io/healthcheck-path:\"/\"alb.ingress.kubernetes.io/success-codes:\"302\"Troubleshooting and make sure ALB is created:\nwatch kubectl -n kube-system get all Find out controller name in pod. In this case - pod/aws-load-balancer-controller-98f66dcb8-zkz8k\nMake sure all are up.\nCheck logs:\nkubectl logs -n kube-system aws-load-balancer-controller-98f66dcb8-zkz8k When updating ALB is often doesn’t update properly so you will need to delete and recreate the ALB:\nkubectl delete deployment -n kube-system aws-load-balancer-controller helm upgrade -i aws-load-balancer-controller eks/aws-load-balancer-controller --set clusterName=xnat --set serviceAccount.create=false --set serviceAccount.name=aws-load-balancer-controller -n kube-system Change the stickiness of the Load Balancer:\nIt is important to set a stickiness time on the load balancer or you can get an issue where the Database thinks you have logged in but the pod you connect to knows you haven’t so you can’t login. Setting stickiness reasonably high – say 30 minutes, can get round this.\nalb.ingress.kubernetes.io/target-group-attributes:stickiness.enabled=true,stickiness.lb_cookie.duration_seconds=1800Change the Load Balancing Algorithm:\nalb.ingress.kubernetes.io/target-group-attributes:load_balancing.algorithm.type=least_outstanding_requestsAdd SSL encryption to your Application Load Balancer Firstly, you need to add an SSL certificate to your ALB annotations. Kubernetes has a built in module: Cert Manager, to deal with cross clouds / infrastructure.\nhttps://cert-manager.io/docs/installation/kubernetes/\nHowever, in this case, AWS has a built in Certificate Manager that creates and a renews SSL certificates for free so we will be using this technology.\nYou can read more about it here:\nhttps://aws.amazon.com/certificate-manager/getting-started/#:~:text=To%20get%20started%20with%20ACM,certificate%20from%20your%20Private%20CA.\nThis assumes you have a valid certificate created through AWS Certificate Manager and you know the ARN.\nThese are additional annotations to add to values file and explanations above:\nListen on port 80 and 443:\nalb.ingress.kubernetes.io/listen-ports:'[{\"HTTP\": 80}, {\"HTTPS\":443}]'Specify the ARN of your SSL certificate from AWS Certificate Manager (change for your actual ARN):\nalb.ingress.kubernetes.io/certificate-arn:\"arn:aws:acm:XXXXXXX:certificate/XXXXXX\"Specify AWS SSL Policy:\nalb.ingress.kubernetes.io/ssl-policy:\"ELBSecurityPolicy-TLS-1-2-Ext-2018-06\"For more details see here of SSL policy options:\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/application/create-https-listener.html\nFinally, for this to successfully work you need to change the host path to allow any path or the Tomcat URL will be sent to a 404 by the Load Balancer. Put a wildcard in the paths to allow any eventual URL (starting with xnat.example.com in this case):\nhosts:- host:xnat.example.compaths:[\"/*\"]Redirect HTTP to HTTPS: This does not work on Kubernetes 1.19 or above as the “use-annotation” command does not work. There is seemingly no documentation on the required annotations to make this work.\nAdd the following annotation to your values file below the ports to listen on (see above):\nalb.ingress.kubernetes.io/actions.ssl-redirect:'{\"Type\": \"redirect\", \"RedirectConfig\": {\"Protocol\": \"HTTPS\", \"Port\": \"443\", \"StatusCode\": \"HTTP_301\"}}'You must then update the Rules section of ingress.yaml found within the releases/xnat/charts/xnat-web/templates directory to look like this:\nrules:{{- range .Values.ingress.hosts }}- host:{{.host | quote }}http:paths:{{- range .paths }}- path:{{.path }}backend:serviceName:{{$fullName }}servicePort:{{$svcPort }}{{- end }}{{- end }}This will redirect HTTP to HTTPS on Kubernetes 1.18 and below.\nFull values.yaml file ingress section:\ningress:enabled:trueannotations:kubernetes.io/ingress.class:albalb.ingress.kubernetes.io/scheme:internet-facingalb.ingress.kubernetes.io/target-type:ipalb.ingress.kubernetes.io/listen-ports:'[{\"HTTP\": 80}, {\"HTTPS\":443}]'alb.ingress.kubernetes.io/actions.ssl-redirect:'{\"Type\": \"redirect\", \"RedirectConfig\": {\"Protocol\": \"HTTPS\", \"Port\": \"443\", \"StatusCode\": \"HTTP_301\"}}'alb.ingress.kubernetes.io/healthcheck-path:\"/\"alb.ingress.kubernetes.io/success-codes:\"302\"alb.ingress.kubernetes.io/certificate-arn:\"arn:aws:acm:XXXXXXX:certificate/XXXXXX\"alb.ingress.kubernetes.io/ssl-policy:\"ELBSecurityPolicy-TLS-1-2-Ext-2018-06\"alb.ingress.kubernetes.io/target-group-attributes:\"stickiness.enabled=true,stickiness.lb_cookie.duration_seconds=1800,load_balancing.algorithm.type=least_outstanding_requests\"Further Reading:\n https://medium.com/devops-dudes/running-the-latest-aws-load-balancer-controller-in-your-aws-eks-cluster-9d59cdc1db98  Troubleshooting EKS Load Balancers:\n https://aws.amazon.com/premiumsupport/knowledge-center/eks-load-balancers-troubleshooting/ https://medium.com/@ManagedKube/kubernetes-troubleshooting-ingress-and-services-traffic-flows-547ea867b120  ALB annotations:\n https://kubernetes-sigs.github.io/aws-load-balancer-controller/guide/ingress/annotations/  ","categories":"","description":"","excerpt":"Creating an Application Load Balancer to connect to the AIS Helm chart …","ref":"/docs/charts/deployment/alb-ingress-controller/","tags":"","title":"ALB Ingress Controller"},{"body":"There are three types of autoscaling that Kubernetes offers:\n  Horizontal Pod Autoscaling\nHorizontal Pod Autoscaling (HPA) is a technology that scales up or down the number of replica pods for an application based on resource limits specified in a values file.\n  Vertical Pod Autoscaling\nVertical Pod Autoscaling (VPA) increases or decreases the resources to each pod when it gets to a certain percentage to help you best deal with your resources. After some testing this is legacy and HPA is preferred and also built into the Helm chart so we won’t be utilising this technology.\n  Cluster-autoscaling\nCluster-autoscaling is where the Kubernetes cluster itself spins up or down new Nodes (think EC2 instances in this case) to handle capacity.\n  You can’t use HPA and VPA together so we will use HPA and Cluster-Autoscaling. Prerequisites  Running Kubernetes Cluster and XNAT Helm Chart AIS Deployment AWS Application Load Balancer (ALB) as an Ingress Controller with some specific annotations Resources (requests and limits) need to specified in your values file Metrics Server Cluster-Autoscaler  \nYou can find more information on applying ALB implementation for the AIS Helm Chart deployment in the ALB-Ingress-Controller document in this repo, so will not be covering that here, save to say there are some specific annotations that are required for autoscaling to work effectively.\nSpecific annotations required:\nalb.ingress.kubernetes.io/target-group-attributes:\"stickiness.enabled=true,stickiness.lb_cookie.duration_seconds=1800,load_balancing.algorithm.type=least_outstanding_requests\"alb.ingress.kubernetes.io/target-type:ipLet’s breakdown and explain the sections.\nChange the stickiness of the Load Balancer:\nIt is important to set a stickiness time on the load balancer. This forces you to the same pod all the time and retains your session information. Without stickiness, after logging in, the Database thinks you have logged but the Load Balancer can alternate which pod you go to. The session details are kept on each pod so the new pod thinks you aren’t logged in and keeps logging you out all the time. Setting stickiness time reasonably high – say 30 minutes, can get round this.\nstickiness.enabled=true,stickiness.lb_cookie.duration_seconds=1800Change the Load Balancing Algorithm for best performance:\nload_balancing.algorithm.type=least_outstanding_requestsChange the Target type:\nNot sure why but if target-type is set to instance and not ip, it disregards the stickiness rules.\nalb.ingress.kubernetes.io/target-type:ip\nResources (requests and limits) need to specified in your values file In order for HPA and Cluster-autoscaling to work, you need to specify resources - requests and limits, in the AIS Helm chart values file, or it won’t know when to scale.\nThis makes sense because how can you know when you are running out of resources to start scaling up if you don’t know what your resources are to start with?\nIn your values file add the following lines below the xnat-web section (please adjust the CPU and memory to fit with your environment):\nresources:limits:cpu:1000mmemory:3000Mirequests:cpu:1000mmemory:3000MiYou can read more about what this means here:\nhttps://kubernetes.io/docs/tasks/configure-pod-container/assign-cpu-resource/\nFrom my research with HPA, I discovered a few important facts.\n Horizontal Pod Autoscaler doesn’t care about limits, it bases autoscaling on requests. Requests are meant to be the minimum needed to safely run a pod and limits are the maximum. However, this is completely irrelevant for HPA as it ignores the limits altogether so I specify the same resources for requests and limits. See this issue for more details:  https://github.com/kubernetes/kubernetes/issues/72811\nXNAT is extremely memory hungry, and any pod will use approximately 750MB of RAM without doing anything. This is important as when the requests are set below that, you will have a lot of pods scale up, then scale down and no consistency for the user experience. This will play havoc with user sessions and annoy everyone a lot. Applications - specifically XNAT Desktop can use a LOT of memory for large uploads (I have seen 12GB RAM used on an instance) so try and specify as much RAM as you can for the instances you have. In the example above I have specified 3000MB of RAM and 1 vCPU. The worker node instance has 4 vCPUs and 4GB. You would obviously use larger instances if you can. You will have to do some testing to work out the best Pod to Instance ratio for your environment.  \nMetrics Server Download the latest Kubernetes Metrics server yaml file. We will need to edit it before applying the configuration or HPA won’t be able to see what resources are being used and none of this will work.\nwget https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml Add the following line:\n- --kubelet-insecure-tlsto here:\nspec:containers:- args:Completed section should look like this:\nspec:containers:- args:- --kubelet-insecure-tls- --kubelet-preferred-address-types=InternalIP,ExternalIP- --cert-dir=/tmp- --secure-port=443- --kubelet-use-node-status-port- --metric-resolution=15sNow apply it to your Cluster:\nk -nkube-system apply -f components.yaml Congratulations - you now have an up and running Metrics server.\nYou can read more about Metrics Server here:\nhttps://github.com/kubernetes-sigs/metrics-server \nCluster-Autoscaler There are quite a lot of ways to use the Cluster-autoscaler - single zone node clusters deployed in single availability zones (no AZ redundancy), single zone node clusters deployed in multiple Availability zones or single Cluster-autoscalers that deploy in multiple Availability Zones. In this example we will be deploying the autoscaler in multiple Availability Zones (AZ’s).\nIn order to do this, a change needs to be made to the StorageClass configuration used.\nDelete whatever StorageClasses you have and then recreate them changing the VolumeBindingMode. At a minimum you will need to change the GP2 / EBS StorageClass VolumeBindingMode but if you are using a persistent volume for archive / prearchive, that will also need to be updated.\nChange this:\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:gp2annotations:storageclass.kubernetes.io/is-default-class:\"true\"provisioner:kubernetes.io/aws-ebsvolumeBindingMode:Immediateparameters:fsType:ext4type:gp2to this:\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:gp2annotations:storageclass.kubernetes.io/is-default-class:\"true\"provisioner:kubernetes.io/aws-ebsvolumeBindingMode:WaitForFirstConsumerparameters:fsType:ext4type:gp2The run the following commands (assuming the file above is called storageclass.yaml):\nkubectl delete sc --all kubectl apply -f storageclass.yaml This stops pods trying to bind to volumes in different AZ’s.\nYou can read more about this here:\nhttps://aws.amazon.com/blogs/containers/amazon-eks-cluster-multi-zone-auto-scaling-groups/\nRelevant section: If you need to run a single ASG spanning multiple AZs and still need to use EBS volumes you may want to change the default VolumeBindingMode to WaitForFirstConsumer as described in the documentation here. Changing this setting “will delay the binding and provisioning of a PersistentVolume until a pod using the PersistentVolumeClaim is created.” This will allow a PVC to be created in the same AZ as a pod that consumes it.\nIf a pod is descheduled, deleted and recreated, or an instance where the pod was running is terminated then WaitForFirstConsumer won’t help because it only applies to the first pod that consumes a volume. When a pod reuses an existing EBS volume there is still a chance that the pod will be scheduled in an AZ where the EBS volume doesn’t exist.\nYou can refer to AWS documentation for how to install the EKS Cluster-autoscaler:\nhttps://docs.aws.amazon.com/eks/latest/userguide/cluster-autoscaler.html\nThis is specific for your deployment IAM roles, clusternames etc, so will not specified here.\n\nConfigure Horizontal Pod Autoscaler Add the following lines into your values file under the xnat-web section:\nautoscaling:enabled:trueminReplicas:2maxReplicas:100targetCPUUtilizationPercentage:80targetMemoryUtilizationPercentage:80Tailor it your own environment. this will create 2 replicas (pods) at start up, up to a limit of 100 replicas, and will scale up pods when 80% CPU and 80% Memory are utilised - read more about that again here:\nhttps://kubernetes.io/docs/tasks/configure-pod-container/assign-cpu-resource/\nThis is the relevant parts of my environment when running the get command:\nk -nxnat get horizontalpodautoscaler.autoscaling/xnat-xnat-web NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE xnat-xnat-web StatefulSet/xnat-xnat-web 34%/80%, 0%/80% 2 100 2 3h29m As you can see 34% of memory is used and 0% CPU. Example of get command for pods - no restarts and running nicely.\nk -nxnat get pods NAME READY STATUS RESTARTS AGE pod/xnat-xnat-web-0 1/1 Running 0 3h27m pod/xnat-xnat-web-1 1/1 Running 0 3h23m \nTroubleshooting Check Metrics server is working (assuming in the xnat namespace) and see memory and CPU usage:\nkubectl top pods -nxnat kubectl top nodes Check Cluster-Autoscaler logs:\nkubectl logs -f deployment/cluster-autoscaler -n kube-system Check the HPA:\nkubectl -nxnat describe horizontalpodautoscaler.autoscaling/xnat-xnat-web ","categories":"","description":"","excerpt":"There are three types of autoscaling that Kubernetes offers: …","ref":"/docs/charts/operations/autoscaling-xnat-kubernetes-with-eks/","tags":"","title":"Autoscaling XNAT on Kubernetes with EKS"},{"body":"Create an AKS Cluster One of the great things about Azure is the Azure Cli. Specify Bash and then you can run all commands through your web browser and all tools and kubectl / az commands are already installed and available without having to create them on your workstation or spin up a VM instance for the sole purpose of controlling the cluster.\nYou can do this via the console if you want. By Azure cli, see below. Create a resource group first.\nSpecify your Resource Group, cluster name (in our case xnat but please update if your Cluster is name differently), node count and VM instance size:\naz aks create \\ --resource-group \u003cResource Group Name\u003e \\ --name xnat \\ --node-count 3 \\ --generate-ssh-keys \\ --node-vm-size Standard_B2s \\ --enable-managed-identity Get AZ AKS credentials to run kubectl commands against your Cluster\naz aks get-credentials --name xnat --resource-group \u003cResource Group Name\u003e Confirm everything is setup correctly:\nkubectl get nodes -o wide kubectl cluster-info Download and install AIS Chart git clone https://github.com/Australian-Imaging-Service/charts.git Add the AIS repo and update Helm:\nhelm repo add ais https://australian-imaging-service.github.io/charts helm repo update Change to the correct directory and update dependencies. This will download and install the Postgresql Helm Chart. You don’t need to do this if you want to connect to an external Postgresql DB.\ncd ~/charts/release/xnat helm dependency update Create the namespace and install the chart, then watch it be created.\nkubectl create namespace xnat helm upgrade xnat ais/xnat --install -nxnat watch kubectl -nxnat get all It will complain that the Postgresql password is empty and needs updating. Create an override values file (in this case values-aks.yaml but feel free to call it what you wish) and add the following inserting your own desired values:\nxnat-web:postgresql:postgresqlDatabase:\u003cyour database\u003epostgresqlUsername:\u003cyour username\u003epostgresqlPassword:\u003cyour password\u003eUpdate volume / persistence information It turns out that there is an issue with Storage classes that means that the volumes are not created automatically. We need to make a small change to the storageClass configuration for the ReadWriteOnce volumes and create new external volumes for the ReadWriteMany ones.\nFirstly, we create our own Azure files volumes for archive and prearchive and make a slight adjustment to the values configuration and apply as an override.\nFollow this document for the details of how to do that:\nhttps://docs.microsoft.com/en-us/azure/aks/azure-files-volume\nFirstly, export some values that will be used to create the Azure files volumes. Please substitute the details of your environment here.\nAKS_PERS_STORAGE_ACCOUNT_NAME=\u003cyour storage account name\u003e AKS_PERS_RESOURCE_GROUP=\u003cyour resource group\u003e AKS_PERS_LOCATION=\u003cyour region\u003e AKS_PERS_SHARE_NAME=archive-xnat-xnat-web archive-xnat-xnat-web will need to be used or the Helm chart won’t be able to find the mount.\nCreate a storage account:\naz storage account create -n $AKS_PERS_STORAGE_ACCOUNT_NAME -g $AKS_PERS_RESOURCE_GROUP -l $AKS_PERS_LOCATION --sku Standard_LRS Export the connection string as an environment variable, this is used when creating the Azure file share:\nexport AZURE_STORAGE_CONNECTION_STRING=$(az storage account show-connection-string -n $AKS_PERS_STORAGE_ACCOUNT_NAME -g $AKS_PERS_RESOURCE_GROUP -o tsv) Create the file share:\naz storage share create -n $AKS_PERS_SHARE_NAME --connection-string $AZURE_STORAGE_CONNECTION_STRING Get storage account key:\nSTORAGE_KEY=$(az storage account keys list --resource-group $AKS_PERS_RESOURCE_GROUP --account-name $AKS_PERS_STORAGE_ACCOUNT_NAME --query \"[0].value\" -o tsv) Echo storage account name and key:\necho Storage account name: $AKS_PERS_STORAGE_ACCOUNT_NAME echo Storage account key: $STORAGE_KEY Make a note of the Storage account name and key as you will need them.\nNow repeat this process but update the Share name to prearchive-xnat-xnat-web. Run this first and then repeat the rest of the commands:\nAKS_PERS_SHARE_NAME=prearchive-xnat-xnat-web Create a Kubernetes Secret In order to mount the volumes, you need to create a secret. As we have created our Helm chart in the xnat namespace, we need to make sure that is added into the following command (not in the original Microsoft guide):\nkubectl -nxnat create secret generic azure-secret --from-literal=azurestorageaccountname=$AKS_PERS_STORAGE_ACCOUNT_NAME --from-literal=azurestorageaccountkey=$STORAGE_KEY Create Kubernetes Volumes Now we need to create two persistent volumes outside of the Helm Chart which the Chart can mount - hence requiring the exact name.\nCreate two files\n pv_archive.yaml pv_prearchive.yaml   pv_archive.yaml apiVersion:v1kind:PersistentVolumemetadata:name:archive-xnat-xnat-webspec:capacity:storage:10GiaccessModes:- ReadWriteManyclaimRef:name:archive-xnat-xnat-webnamespace:xnatazureFile:secretName:azure-secretshareName:archive-xnat-xnat-webreadOnly:falsemountOptions:- dir_mode=0755- file_mode=0755- uid=1000- gid=1000- mfsymlinks- nobrl   pv_prearchive.yaml apiVersion:v1kind:PersistentVolumemetadata:name:prearchive-xnat-xnat-webspec:capacity:storage:10GiaccessModes:- ReadWriteManyclaimRef:name:prearchive-xnat-xnat-webnamespace:xnatazureFile:secretName:azure-secretshareName:prearchive-xnat-xnat-webreadOnly:falsemountOptions:- dir_mode=0755- file_mode=0755- uid=1000- gid=1000- mfsymlinks- nobrl  Size doesn’t really matter as like EFS, Azure files is completely scaleable. Just make sure it is the same as your values file for those volumes.\nApply the volumes kubectl apply -f pv_archive.yaml kubectl apply -f pv_prearchive.yaml We should now have two newly created volumes our Helm chart can mount.\nUpdate our override values file for our Helm chart. Edit your values-aks.yaml file from above and add the following in (postgresql entries already added):\nPaste the following:\nxnat-web:persistence:cache:accessMode:ReadWriteOncemountPath:/data/xnat/cachestorageClassName:\"\"size:10Giwork:accessMode:ReadWriteOncemountPath:/data/xnat/home/workstorageClassName:\"\"size:1Gilogs:accessMode:ReadWriteOncemountPath:/data/xnat/home/logsstorageClassName:\"\"size:1Giplugins:accessMode:ReadWriteOncemountPath:/data/xnat/home/pluginsstorageClassName:\"\"size:0volumes:archive:accessMode:ReadWriteManymountPath:/data/xnat/archivestorageClassName:\"\"size:10Giprearchive:accessMode:ReadWriteManymountPath:/data/xnat/prearchivestorageClassName:\"\"size:10Gipostgresql:postgresqlDatabase:\u003cyour database\u003epostgresqlUsername:\u003cyour username\u003epostgresqlPassword:\u003cyour password\u003eYou can now apply the helm chart with your override and all the volumes will mount.\nhelm upgrade xnat ais/xnat -i -f values-aks.yaml -nxnat Congratulations! Your should now have a working XNAT environment with properly mounted volumes.\nYou can check everything is working:\nkubectl -nxnat get ev kubectl -nxnat get all kubectl -nxnat get pvc,pv Check that the XNAT service comes up:\nkubectl -nxnat logs xnat-xnat-web-0 -f Create a static public IP, an ingress controller, LetsEncrypt certificates and point it to our Helm chart OK so all good so far but we can’t actually access our XNAT environment from outside of our cluster so we need to create an Ingress Controller.\nYou can follow the URL here from Microsoft for more detailed information:\nhttps://docs.microsoft.com/en-us/azure/aks/ingress-static-ip\nFirst, find out the resource name of the AKS Cluster:\naz aks show --resource-group \u003cyour resource group\u003e --name \u003cyour cluster name\u003e --query nodeResourceGroup -o tsv This will create the output for your next command.\naz network public-ip create --resource-group \u003coutput from previous command\u003e --name \u003ca name for your public IP\u003e --sku Standard --allocation-method static --query publicIp.ipAddress -o tsv Point your FQDN to the public IP address you created For the Letsencrypt certificate issuer to work it needs to be based on a working FQDN (fully qualified domain name), so in whatever DNS manager you use, create a new A record and point your xnat FQDN (xnat.example.com for example) to the IP address you just created.\nAdd the ingress-nginx repo:\nhelm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx Now create the ingress controller with a DNS Label (doesn’t need to be FQDN here) and the IP created in the last command:\nhelm install nginx-ingress ingress-nginx/ingress-nginx --namespace xnat --set controller.replicaCount=2 --set controller.nodeSelector.\"beta\\.kubernetes\\.io/os\"=linux --set defaultBackend.nodeSelector.\"beta\\.kubernetes\\.io/os\"=linux --set controller.admissionWebhooks.patch.nodeSelector.\"beta\\.kubernetes\\.io/os\"=linux --set controller.service.loadBalancerIP=\"1.2.3.4\" --set controller.service.annotations.\"service\\.beta\\.kubernetes\\.io/azure-dns-label-name\"=\"xnat-aks\" Please ensure to update the details above to suit your environment - including namespace.\nInstall Cert-Manager and attach to the Helm chart and Ingress Controller kubectl label namespace xnat cert-manager.io/disable-validation=true helm repo add jetstack https://charts.jetstack.io helm repo update helm install cert-manager --namespace xnat --version v1.3.1 --set installCRDs=true --set nodeSelector.\"beta\\.kubernetes\\.io/os\"=linux jetstack/cert-manager You can find a write up of these commands and what they do in the Microsoft article.\nCreate a cluster-issuer.yaml to issue the Letsencrypt certificates  cluster-issuer.yaml apiVersion:cert-manager.io/v1alpha2kind:ClusterIssuermetadata:name:letsencrypt-prodspec:acme:server:https://acme-v02.api.letsencrypt.org/directoryemail:your@emailaddress.comprivateKeySecretRef:name:letsencrypt-prodsolvers:- http01:ingress:class:nginxpodTemplate:spec:nodeSelector:\"kubernetes.io/os\": linux  In our case, we want production Letsencrypt certificates hence letsencrypt-prod (mentioned twice here and in values-aks.yaml). If you are doing testing you can use letsencrypt-staging. See Microsoft article for more details.\nPlease do not forget to use your email address here.\nApply the yaml file:\nkubectl apply -f cluster-issuer.yaml -nxnat Update your override values file to point to your ingress controller and Letsencrypt Cluster issuer Add the following to your values-aks.yaml file (I have added the volume and postgresql details as well for the complete values file):\n values-aks.yaml xnat-web:ingress:enabled:trueannotations:kubernetes.io/ingress.class:nginxcert-manager.io/cluster-issuer:letsencrypt-prodtls:- hosts:- \"yourxnat.example.com\"secretName:tls-secrethosts:- \"yourxnat.example.com\"rules:- host:\"yourxnat.example.com\"http:paths:- path:\"/\"backend:serviceName:\"xnat-xnat-web\"servicePort:80persistence:cache:accessMode:ReadWriteOncemountPath:/data/xnat/cachestorageClassName:\"\"size:10Giwork:accessMode:ReadWriteOncemountPath:/data/xnat/home/workstorageClassName:\"\"size:1Gilogs:accessMode:ReadWriteOncemountPath:/data/xnat/home/logsstorageClassName:\"\"size:1Giplugins:accessMode:ReadWriteOncemountPath:/data/xnat/home/pluginsstorageClassName:\"\"size:0volumes:archive:accessMode:ReadWriteManymountPath:/data/xnat/archivestorageClassName:\"\"size:10Giprearchive:accessMode:ReadWriteManymountPath:/data/xnat/prearchivestorageClassName:\"\"size:10Gipostgresql:postgresqlDatabase:\u003cyour database\u003epostgresqlUsername:\u003cyour username\u003epostgresqlPassword:\u003cyour password\u003e  Change yourxnat.example.com to whatever you want your XNAT FQDN to be.\nIf you are using Letsencrypt-staging, update the cert-manager.io annotation accordingly.\nNow update your helm chart and you should now have a fully working Azure XNAT installation with HTTPS redirection enabled, working volumes and fully automated certificates with automatic renewal.\nhelm upgrade xnat ais/xnat -i -f values-aks.yaml -nxnat ","categories":"","description":"","excerpt":"Create an AKS Cluster One of the great things about Azure is the Azure …","ref":"/docs/charts/deployment/azure-setup-full/","tags":"","title":"Azure Setup Full"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/charts/","tags":"","title":"Charts"},{"body":"Tools    Name Description Use     Kind Tool for running local Kubernetes clusters using Docker container “nodes” Testing chart functionality    ","categories":"","description":"","excerpt":"Tools    Name Description Use     Kind Tool for running local …","ref":"/docs/charts/development/cicd/","tags":"","title":"Continuous Integration / Continuous Delivery"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/charts/deployment/","tags":"","title":"Deployment"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/charts/development/","tags":"","title":"Development"},{"body":"Setting up Docker Swarm A complete explanation of how to setup Docker Swarm is outside the scope of this document but you can find some useful articles here:\nhttps://scalified.com/2018/10/08/building-jenkins-pipelines-docker-swarm/\nhttps://docs.docker.com/engine/swarm/swarm-tutorial/create-swarm/\nhttps://docs.docker.com/engine/swarm/ingress/\nSetting up with AWS:\nhttps://semaphoreci.com/community/tutorials/bootstrapping-a-docker-swarm-mode-cluster\nPipelines XNAT uses pipelines to perform various different processes - mostly converting image types to other image types (DICOM to NIFTI for example).\nIn the past this was handled on the instance as part of the XNAT program, then as a docker server on the instance and finally, externally as an external docker server, either directly or using Docker swarm.\nXNAT utilises the Container service which is a plugin to perform docker based pipelines. In the case of Kubernetes, docker MUST be run externally so Docker swarm is used as it provides load balancing.\nWhilst the XNAT team work on replacing the Container service on Docker Swarm with a Kubernetes based Container service, Docker swarm is the most appropriate stop gap option.\nPrerequisites You will require the Docker API endpoint opened remotely so that XNAT can access and send pipeline jobs to it. For security, this should be done via HTTPS (not HTTP).\nStandard port is TCP 2376. With Docker Swarm enabled you can send jobs to any of the manager or worker nodes and it will automatically internally load balance. I chose to use the Manager node’s IP and pointed DNS to it.\nYou should lock access to port 2376 to the Kubernetes XNAT subnets only using firewalls or Security Group settings. You can also use an external Load balancer with certificates which maybe preferred.\nIf the certificates are not provided by a known CA, you will need to add the certificates (server, CA and client) to your XNAT container build so choosing a proper certificate from a known CA will make your life easier.\nIf you do use self signed certificates, you will need create a folder, add the certificates and then specify that folder in the XNAT GUI \u003e Administer \u003e Plugin Settings \u003e Container Server Setup \u003e Edit Host Name. In our example case:\nCertificate Path: /usr/local/tomcat/certs Access from the Docker Swarm to the XNAT shared filesystem - at a minimum Archive and build. The AIS Helm chart doesn’t have /data/xnat/build setup by default but without this Docker Swarm can’t write the temporaray files it needs and fails.\nSetup DNS and external certificates Whether you will need to create self signed certificates or public CA verified ones, you will need a fully qualified domain name to create them against.\nI suggest you set an A record to point to the Manager node IP address, or a Load Balancer which points to all nodes. Then create the certificates against your FQDN - e.g. swarm.example.com.\nAllow remote access to Docker API endpoint on TCP 2376 To enable docker to listen on port 2376 edit the service file or create /etc/docker/daemon.json.\nWe will edit the docker service file. Remember to specify whatever certificates you will be using in here. They will be pointing to your FQDN - in our case above, swarm.example.com.\nsystemctl edit docker [Service] ExecStart= ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 --tlsverify --tlscacert /root/.docker/ca.pem --tlscert /root/.docker/server-cert.pem -tlskey /root/.docker/server-key.pem -H unix:///var/run/docker.sock systemctl restart docker Repeat on all nodes. Docker Swarm is now listening remotely on TCP 2376.\nSecure access to TCP port 2376 Add a firewall rule to only allow access to TCP port 2376 from the Kubernetes subnets.\nEnsure Docker Swarm nodes have access to the XNAT shared filesystem Without access to the Archive shared filesystem Docker cannot run any pipeline conversions. This seems pretty obvious. Less obvious however is that the XNAT Docker Swarm requires access to the Build shared filesystem to run temporary jobs before writing back to Archive upon completion.\nThis presents a problem as the AIS Helm Chart does not come with a persistent volume for the Build directory, so we need to create one.\nCreate a volume outside the Helm Chart and then present it in your values file. In this example I created a custom class. Make sure accessMode is ReadWriteMany so Docker Swarm nodes can access.\n volumes: build: accessMode: ReadWriteMany mountPath: /data/xnat/build storageClassName: \"custom-class\" volumeMode: Filesystem persistentVolumeReclaimPolicy: Retain persistentVolumeClaim: claimName: \"build-xnat-xnat-web\" size: 10Gi You would need to create the custom-class storageclass and apply it first or the volume won’t be created. In this case, create a file - storageclass.yaml and add the followinng contents:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: custom-class provisioner: kubernetes.io/no-provisioner volumeBindingMode: WaitForFirstConsumer You can then apply it:\nkubectl apply -f storageclass.yaml Of course you may want to use an existing Storage Class so this maybe unnecessary, it is just an example.\nApply the Kubernetes volume file first and then apply the Helm chart and values file. You should now see something like the following:\nkubectl get -nxnat pvc,pv NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE persistentvolumeclaim/archive-xnat-xnat-web Bound archive-xnat-xnat-web 10Gi RWX custom-class 5d1h persistentvolumeclaim/build-xnat-xnat-web Bound build-xnat-xnat-web 10Gi RWX custom-class 5d1h persistentvolumeclaim/cache-xnat-xnat-web-0 Bound pvc-b5b72b92-d15f-4a22-9b88-850bd726d1e2 10Gi RWO gp2 5d1h persistentvolumeclaim/prearchive-xnat-xnat-web Bound prearchive-xnat-xnat-web 10Gi RWX custom-class 5d1h NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE persistentvolume/archive-xnat-xnat-web 10Gi RWX Retain Bound xnat/archive-xnat-xnat-web custom-class 5d1h persistentvolume/build-xnat-xnat-web 10Gi RWX Retain Bound xnat/build-xnat-xnat-web custom-class 5d1h persistentvolume/prearchive-xnat-xnat-web 10Gi RWX Retain Bound xnat/prearchive-xnat-xnat-web custom-class 5d1h persistentvolume/pvc-b5b72b92-d15f-4a22-9b88-850bd726d1e2 10Gi RWO Delete Bound xnat/cache-xnat-xnat-web-0 gp2 5d1h As you can see, the build directory is now a mounted volume. You are now ready to mount the volumes on the Docker swarm nodes.\nDepending how you presented your shared filesystem, just create the directories on the Docker swarm nodes and manager (if the manager is also a worker), add to /etc/fstab and mount the volumes.\nTo make your life easier use the same file structure for the mounts - i.e build volume mounted in /data/xnat/build and archive volume mounted in /data/xnat/archive. If you don’t do this you will need to specify the Docker swarm mounted XNAT directories in the XNAT GUI.\nAdd your Docker Swarm to XNAT Plugin Settings You can read about the various options in the official XNAT documentation on their website here:\nhttps://wiki.xnat.org/container-service/installing-and-enabling-the-container-service-in-xnat-126156821.html\nhttps://wiki.xnat.org/container-service/configuring-a-container-host-126156926.html\nIn the XNAT GUI, go to Administer \u003e Plugin Settings \u003e Container Server Setup and under Docker Server setup select \u003e New Container host.\nIn our above example, for host name you would select swarm.example.com, URL would be https://swarm.example.com:2376 and certificate path would be /usr/local/tomcat/certs. As previously mentioned, it is desirable to have public CA and certificates to avoid the needs for specifying certificates at all here.\nSelect Swarm Mode to “ON”.\nYou will need to select Path Translation if you DIDN’T mount the Docker swarm XNAT directories in the same place.\nThe other options are optional.\nOnce applied make sure that Status is “Up”. The Image hosts section should also now have a status of Up.\nYou can now start adding your Images \u0026 Commands in the Administer \u003e Plugin Settings \u003e Images \u0026 Commands section.\nTroubleshooting If you have configured docker swarm to listen on port 2376 but status says down, firstly check you can telnet or netcat to the port first locally, then remotely. From one of the nodes:\nnc -zv 127.0.0.1 2376 or\ntelnet 127.0.0.1 2376 If you can, try remotely from a location that has firewall ingress access. In our example previously, try:\nnc -zv swarm.example.com 2376 telnet swarm.example.com 2376 Make sure the correct ports are open and accessible on the Docker swarm manager:\nThe network ports required for a Docker Swarm to function correctly are:\nTCP port 2376 for secure Docker client communication. This port is required for Docker Machine to work. Docker Machine is used to orchestrate Docker hosts.\nTCP port 2377. This port is used for communication between the nodes of a Docker Swarm or cluster. It only needs to be opened on manager nodes.\nTCP and UDP port 7946 for communication among nodes (container network discovery).\nUDP port 4789 for overlay network traffic (container ingress networking).\nMake sure docker service is started on all docker swarm nodes.\nIf Status is set to Up and the container automations are failing, confirm the archive AND build shared filesystems are properly mounted on all servers - XNAT and Docker swarm. A Failed (Rejected) status for a pipeline is likely due to this error.\nIn this case, as a service can’t be created you won’t have enough time to see the service logs with the usual:\ndocker service ls command followed by looking at the service in question, so stop the docker service on the Docker swarm node and start in the foreground, using our service example above:\ndockerd -H tcp://0.0.0.0:2376 --tlsverify --tlscacert /root/.docker/ca.pem --tlscert /root/.docker/server-cert.pem --tlskey /root/.docker/server-key.pem -H unix:///var/run/docker.sock Then upload some dicoms and watch the processing run in the foreground.\nDocker Swarm admin guide:\nhttps://docs.docker.com/engine/swarm/admin_guide/\n","categories":"","description":"","excerpt":"Setting up Docker Swarm A complete explanation of how to setup Docker …","ref":"/docs/charts/operations/docker-swarm-with-xnat/","tags":"","title":"Docker-Swarm-with-XNAT"},{"body":"Previewing documentation locally   Install Hugo (extended version)\nhttps://gohugo.io/getting-started/installing/#quick-install\n  Install PostCSS\nThis is a requirement of Docsy, the Hugo theme we use.\nsudo npm install -D --save autoprefixer sudo npm install -D --save postcss-cli   Ensure the main docuemtation repository Australian-Imaging-Service.github.io was cloned with submodules:\ngit clone --recurse-submodules https://github.com/Australian-Imaging-Service/Australian-Imaging-Service.github.io or by initializing them after a normal clone:\ngit submodule update --init   Clone the other AIS repositories containing documentation side by side\nRepositories containing documentation can be referenced from the go.mod file in the Australian-Imaging-Service.github.io repository.\n$ tree . ├── Australian-Imaging-Service.github.io [snip] ├── charts [snip] ...   Start the live preview within the Australian-Imaging-Service.github.io directory\nhugo serve This will start serving the documentation locally at http://localhost:1313/\n  Adding and updating AIS documentation NB: This is for Public facing documents.\nAll MarkDown documents located under the docs folder in the root of their respective repo will be published to a public site at https://australian-imaging-service.github.io/.\nIf you are adding a new document create a file with a .md extension.\nAdd a YAML front matter block:\n---title:\"Long Page Title\"linkTitle:\"Short Nav Title\"weight:100description:\u003e-Page description for heading and indexes.---add your markdown hereThe front matter YAML will tell Hugo that this is a page to publish and the Title to use in links and page headings.\nweight affects the placement of the page within the navigation sidebar.\nThats it!\nOnly documentation within the master or main branch will be published to the official AIS documentation.  ","categories":"","description":"","excerpt":"Previewing documentation locally   Install Hugo (extended version) …","ref":"/docs/contributing/documentation/","tags":"","title":"Documentation"},{"body":"Connecting AIS XNAT Helm Deployment to an External Postgresql Database By default, the AIS XNAT Helm Deployment creates a Postgresql database in a separate pod to be run locally on the cluster.\nIf the deployment is destroyed the data in the database is lost. This is fine for testing purposes but unsuitable for a production environment.\nLuckily a mechanism was put into the Helm template to allow connecting to an External Postgresql Database.\nUpdating Helm charts values files to point to an external Database Firstly, clone the AIS Charts Helm template:\ngit clone https://github.com/Australian-Imaging-Service/charts.git values-dev.yaml This file is located in charts/releases/xnat\nCurrent default configuration:\nglobal:postgresql:postgresqlPassword:\"xnat\"postgresqlEnabled:truepostgresqlExternalName:\"\"postgresqlExternalIPs:- 139.95.25.8- 130.95.25.9this line:\npostgresqlEnabled: true\nNeeds to be changed to false to disable creation of the Postgresql pod and create an external database connection.\nThe other details are relatively straightforward - Generally you would only specify either:\npostgresqlExternalName or postgresqlExternalIPs\npostgresqlPassword will be your database user password.\nAn example configuration using a sample AWS RDS instance would look like this:\nglobal:postgresql:postgresqlPassword:\"yourpassword\"postgresqlEnabled:falsepostgresqlExternalName:\"xnat.randomstring.ap-southeast-2.rds.amazonaws.com\"Top level values.yaml This file is also located in charts/releases/xnat\nCurrent default configuration:\nglobal:postgresql:postgresqlDatabase:\"xnat\"postgresqlUsername:\"xnat\"#postgresqlPassword: \"\"#servicePort: \"\"postgresqlEnabled:truepostgresqlExternalName:\"\"postgresqlExternalIPs:[]An example configuration using a sample AWS RDS instance would look like this:\nglobal:postgresql:postgresqlDatabase:\"yourdatabase\"postgresqlUsername:\"yourusername\"postgresqlPassword:\"yourpassword\"postgresqlEnabled:falsepostgresqlExternalName:\"xnat.randomstring.ap-southeast-2.rds.amazonaws.com\"Please change the database, username, password and External DNS (or IP) details to match your environment.\nxnat-web values.yaml This file is also located in charts/releases/xnat/charts/xnat-web\nCurrent default configuration:\npostgresql:postgresqlDatabase:\"xnat\"postgresqlUsername:\"xnat\"postgresqlPassword:\"xnat\"Change to match your environment as with the other values.yaml.\nYou should now be able to connect your XNAT application Kubernetes deployment to your external Postgresql DB to provide a suitable environment for production.\nFor more details about deployment have a look at the README.md here:\nhttps://github.com/Australian-Imaging-Service/charts/tree/main/releases/xnat\nCreating an encrypted connection to an external Postgresql Database The database connection string for XNAT is found in the XNAT home directory - usually\n/data/xnat/home/config/xnat-conf.properties\nBy default the connection is unencrypted. If you wish to encrypt this connection you must append to the end of the Database connection string.\nUsual string:\ndatasource.url=jdbc:postgresql://xnat-postgresql/yourdatabase\nOptions:    Option Description     ssl=true use SSL encryption   sslmode=require require SSL encryption   sslfactory=org.postgresql.ssl.NonValidatingFactory Do not require validation of Certificate Authority    The last option is useful as otherwise you will need to import the CA cert into your Java keystone on the docker container.\nThis means updating and rebuilding the XNAT docker image before being deployed to the Kubernetes Pod and this can be impractical.\nComplete string would look like this ( all on one line):\ndatasource.url=jdbc:postgresql://xnat-postgresql/yourdatabase?ssl=true\u0026sslmode=require\u0026sslfactory=org.postgresql.ssl.NonValidatingFactory\nUpdate your Helm Configuration: Update the following line in charts/releases/xnat/charts/xnat-web/templates/secrets.yaml from:\ndatasource.url=jdbc:postgresql://{{ template \"xnat-web.postgresql.fullname\" . }}/{{ template \"xnat-web.postgresql.postgresqlDatabase\" . }}\nto:\ndatasource.url=jdbc:postgresql://{{ template \"xnat-web.postgresql.fullname\" . }}/{{ template \"xnat-web.postgresql.postgresqlDatabase\" . }}?ssl=true\u0026sslmode=require\u0026sslfactory=org.postgresql.ssl.NonValidatingFactory\nThen deploy / redeploy.\nIt should be noted that the Database you are connecting to needs to be encrypted in the first place for this to be successful.\nThis is outside the scope of this document.\n ","categories":"","description":"","excerpt":"Connecting AIS XNAT Helm Deployment to an External Postgresql Database …","ref":"/docs/charts/operations/external-pgsql-db-connection/","tags":"","title":"External-PGSQL-DB-Connection"},{"body":"Setup   Copy the .github and docs folder to the target repository from skeleton\n  Edit the gh-pages.yml workflow in the main docs repository\nAdd a new checkout step for the repository, replacing REPOSITORY_NAME with the name of the repository\n28 29 30 31 32  - name:Checkout REPOSITORY_NAME repouses:actions/checkout@v2with:repository:'Australian-Imaging-Service/REPOSITORY_NAME'path:REPOSITORY_NAME    Edit the config.toml file in the main docs repository\nAdd a new module import section to the end of the file, replacing REPOSITORY_NAME with the name of the repository\n101 102 103  [[module.imports]] path = \"github.com/Australian-Imaging-Service/REPOSITORY_NAME/docs\" disable = false     Edit the go.mod file in the main docs repository\nAdd a reference to the new repository, replacing REPOSITORY_NAME with the name of the repository\nreplace github.com/Australian-Imaging-Service/REPOSITORY_NAME/docs =\u003e ../REPOSITORY_NAME/docs require github.com/Australian-Imaging-Service/REPOSITORY_NAME/docs v0.0.0   Troubleshooting Error: Parameter token or opts.auth is required Usually indicative that either:\n The workflow is running in a fork, actions cannot access organisation secrets in a fork The repository doesn’t have access to the GITHUBPAGES_KEY organisation secret  Contact the Australian-Imaging-Service organisation admin to request the repository be added    ","categories":"","description":"","excerpt":"Setup   Copy the .github and docs folder to the target repository from …","ref":"/docs/contributing/new-repo/","tags":"","title":"Integrating a new repository"},{"body":"Requirements  An enabled hypervisor, either HyperKit or VirtualBox. HyperKit is the default hypervisor backend on MacOS Yosemite or later installed on a 2010 or newer Mac. Administrative access on Mac.  Download, install and setup Multipass There are two ways to install Multipass on MacOS: brew or the installer. Using brew is the simplest:\n$ brew install --cask multipass Check Multipass version which you are running:\n$ multipass version Start a Multipass VM, then install Microk8s Brew is the easiest way to install Microk8s, but it is not so easy to install an older version. At the time of writing, Microk8s latest version v1.20 seems to have problem for Ingress to attach an external IP (127.0.0.1 on Microk8s vm). We recommend manual installation.\n$ multipass launch --name microk8s-vm --cpus 2 --mem 4G --disk 40G Get a shell inside the newly created VM:\nmultipass shell microk8s-vm Install Microk8s v1.19 in the VM:\n$ sudo snap install microk8s --classic --channel=1.19/stable $ sudo iptables -P FORWARD ACCEPT List your Multik8s VM:\n$ multipass list Shutdown the VM\n$ multipass stop microk8s-vm Delete and cleanup the VM:\n$ multipass delete microk8s-vm $ multipass purge ","categories":"","description":"","excerpt":"Requirements  An enabled hypervisor, either HyperKit or VirtualBox. …","ref":"/docs/charts/development/macos-multipass-k8s/","tags":"","title":"Development workstation with Multipass on MacOS"},{"body":" NB: Ansible playbooks and/or roles may be helpful.  microk8s sudo snap install microk8s --classic microk8s enable dns fluentd ingress metrics-server prometheus rbac registry storage # Install and configure the kubectl client sudo snap install kubectl --classic # Start running more than one cluster and you will be glad you did these steps microk8s config |sed 's/\\(user\\|name\\): admin/\\1: microk8s-admin/' \u003e${HOME}/.kube/microk8s.config # On Mac, use below to set up the admin user # microk8s config |sed 's/\\([user\\|name]\\): admin/\\1: microk8s-admin/' \u003e${HOME}/.kube/microk8s.config cat \u003e\u003e${HOME}/.profile \u003c\u003c'EOT' DIR=\"${HOME}/.kube\" if [ -d \"${DIR}\" ]; then KUBECONFIG=\"$(/usr/bin/find $DIR \\( -name 'config' -o -name '*.config' \\) \\( -type f -o -type l \\) -print0 | tr '\\0' ':')\" KUBECONFIG=\"${KUBECONFIG%:}\" export KUBECONFIG fi EOT # logout or run the above code in your current shell to set the KUBECONFIG environment variable kubectl config use-context microk8s If you have an issue with the operation of microk8s microk8s inspect command is you best friend.\nmicrok8s notes To enable a Load Balancer microk8s comes with metalLB and configures Layer2 mode settings by default. You will be asked for an IPv4 block of addresses, ensure that the address block is in the same Layer 2 as your host, unused and reserved for this purpose (you may need to alter your DHCP service). When you are ready perform the following:\n$ microk8s enable metallb  microk8s does not support IPv6 at this time!  ","categories":"","description":"","excerpt":" NB: Ansible playbooks and/or roles may be helpful.  microk8s sudo …","ref":"/docs/charts/development/ubuntu-microk8s/","tags":"","title":"microk8s-Ubuntu"},{"body":"The microCT data needed to be transferred to the RDS before uploading from RDS to XNAT. Both XNAT and RDS will keep a copy of microCT data at the current stage. If RDS storage researches its maximum capacity or budget in the future, the microCT data on RDS will be removed periodically to save the space.\nThe microCT data can be categorized into three types: raw data, temporary data and reconstructed data.\nThe raw data is saved on the acquisition computer, and in the D drive of the reconstruction computer.\nThe temporary data is generated in the reconstruction process including the “corr” and “prev” folder of each scan and saved in the “ct-data” folder. “corr” folder contains all the projection correction files, while “prev” folder contains the single slice preview image of reconstruction. “corr” and “prev” folder will not be uploaded to the RDS and XNAT.\nThe reconstructed data contains the final images for CT reconstruction, and it is saved in the “Results” folder of each scan. Data is uploaded from microCT to RDS daily at 7 p.m.\nAcquisition control screen   The screenshot above is the acquisition control screen. The User field should always leave blank. The Study field should be filled with the XNAT Project ID and Subject Name in XNAT, e.g. 123_SubjectName.\nProject name can’t contain _, this symbol is used to separate the Project ID and Subject Name. Subject Name can contain any numbers of _.\nThe Scan name will become the Session name on XNAT.\nInstrument data hierarchy is:\nZ: (Aquisiton) D: (Reconstruction) D:\\Data\\[Study]\\[Scan]\\* [Study] is [XNAT Project ID]_[Subject] [Scan] is [Date]_[Other]\nMetadata    Instrument Source Logic Config Source XNAT Destination     [Study] [XNAT Project ID]_* - Subject   [Study] *_[Subject]  Subject   [Scan] - - Session   - - TBD [IID]   - - TBD [QCPID]   [Scan] [Date]_* - Date   - - - Time   [Scan].log …User: [Operator]… (Not collecting this for now) - Operator   - - TBD Scanner Name   - - TBD Scanner Type   - - Sydney Imaging Acquisition Site    Data    Original structure Note XNAT structure XNAT File Type      [Scan].ct-sequence  [Scan].zip uct RAW   [Scan].zipuct  [Scan].zip uct RAW   [Scan].mCT  [Scan].zip uct RAW   [Scan].parameters  [Scan].zip uct RAW   [Scan].log  [Scan].zip uct RAW   [Scan].comments  [Scan].zip uct RAW   [Scan]_left.png  [Scan].zip uct RAW   [Scan]_right.png  [Scan].zip uct RAW   [Scan]_top.png  [Scan].zip uct RAW   [Scan]_x-ray-left.png  [Scan].zip uct RAW   [Scan]_x-ray-right.png  [Scan].zip uct RAW   [Scan]_x-ray-top.png  [Scan].zip uct RAW   ct-data/calibration/[Stuff].smv   calibration    ct-data/calibration/[stuff2]_dark.tif   calibration    ct-data/calibration/[stuff2]_white.tif   calibration    ct-data/calibration/geopar.cfg   calibration    ct-data/proj_000_0_log.csv   proj    ct-data/proj_000_0_00_[NNN].tif where NNN is the index  proj    Results/CT_[Scan]_80um.log Not maintaining folder structure CT_[Scan]_80um.log log RECON   Results/CT_[Scan]_80um.nii Not maintaining folder structure CT_[Scan]_80um.nii NIFTI RECON    ","categories":"","description":"","excerpt":"The microCT data needed to be transferred to the RDS before uploading …","ref":"/docs/instruments/rc-milabsu-ct/","tags":"","title":"RC MILabsU-CT"},{"body":"[Series ID] should be used for matching files.\nThe Project field should filled with the XNAT shortcode (EZBooking Project ID) and the Subject field will be filled with the project name.\nNote For the new project created through DashR, the XNAT shortcode should be at least 3 digits/characters and less than 20 digits/characters.\nIf less than 3 digits, zeros should be added at the beginning. For example, XNAT shortcode 044 for EZBooking Project ID 44\n Metadata    Instrument Source Logic Config Source XNAT Destination     (0010,0010) [Project]:* - [Project]   (0010,0010) *:[Subject] - [Subject]   (0008,0020) [YYYYMMDD]  [Session]   (0008,0050) [Subject Accession Number] -    - - 3T: 10.25910/5cf9e65ffa8c4\n7T: 10.25910/5cf9f821b4c94 [IID] TBD   - - TBD [QCPID] TBD   TBC - - Date   Timestamp (0008,0032) TBC - - Time   Acquisition Duration (0018,9073) - - TBD   Protocol Name (0018,1030)\n/Sequence Name (0018,0024) - - TBD   - - - Operator   (0008,1010) Station Name - - Scanner Name   (0008,1090) Manufacturer’s Model Name - - Scanner Type   - - Sydney Imaging Acquisition Site    Data    Original structure Note XNAT structure XNAT File Type     D:\\smis\\dev\\Data\\[Subject ID]\\DICOM\\[Series]\\[1]\\[Series]_[ScanX] DICOM \\Session\\[Series]\\ DICOM   D:\\smis\\dev\\Data\\[Subject ID]\\Image\\[Series]\\[1]\\[Series]_[ScanX] SUR \\Session\\[Series]\\ SUR   D:\\smis\\dev\\Data\\[Subject ID]\\Image\\[Series]\\patient[SubjectID]_scan[Series] PET (log)     D:\\smis\\dev\\Data\\[Subject ID]\\Image\\[Series]\\[Reconstruction]\\[Series]_stuff PET (Reconstructed)     D:\\smis\\dev\\Data\\[Subject ID]\\Spectroscopy\\[Series]\\[Series]_0 Spectroscopy \\Session\\[Series]\\ MRD, SPR   D:\\smis\\dev\\MRD\\[Subject ID]\\Image\\[Series]\\[Series]_0 MRD, RPR, rtv \\Session\\[Series]\\ MRD, RPR   D:\\smis\\dev\\MRD\\[Subject ID]\\Image\\[Series]\\[Series]_0_o MRD, RPR, rtv \\Session\\[Series]\\ MRD, RPR   D:\\smis\\dev\\MRD\\[Subject ID]\\Image\\[Series]\\btable.txt MRD, RPR, rtv \\Session\\[Series]\\ btable   D:\\smis\\dev\\MRD\\[Subject ID]\\Image\\[Series]\\rtable.txt MRD, RPR, rtv \\Session\\[Series]\\ rtable   D:\\smis\\dev\\PetRaw\\[Subject ID]\\[Series]\\pet_[Series] PET (RAW)      ","categories":"","description":"","excerpt":"[Series ID] should be used for matching files.\nThe Project field …","ref":"/docs/instruments/3t-and-7t-mri/","tags":"","title":"MR Solutions 3T \u0026 7T MRI"},{"body":"NixOS + Minikube # Configure environment cat \u003c\u003cEOF \u003e default.nix { pkgs ? import \u003cnixpkgs\u003e {} }: pkgs.mkShell { buildInputs = with pkgs; [ minikube kubernetes-helm jq ]; shellHook = '' alias kubectl='minikube kubectl' . \u003c(minikube completion bash) . \u003c(helm completion bash) # kubectk and docker completion require the control plane to be running if [ $(minikube status -o json | jq -r .Host) = \"Running\" ]; then . \u003c(kubectl completion bash) . \u003c(minikube -p minikube docker-env) fi ''; } EOF nix-shell minikube start # Will block the terminal, will need to open a new one minikube dashboard # Creates \"default-http-backend\" minikube addons enable ingress ","categories":"","description":"","excerpt":"NixOS + Minikube # Configure environment cat \u003c\u003cEOF \u003e default.nix { …","ref":"/docs/charts/development/nixos-minikube/","tags":"","title":"NixOS + Minikube"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/charts/operations/","tags":"","title":"Operations"},{"body":"Requirements and rationals   Collaboration and knowledge share\nTool selection has been chosen with a security oriented focus but enabling collaboration and sharing of site specific configurations, experiences and recommendations.\n  Security\nA layered security approach with mechanisms to provide access at granular levels either through Access Control Lists (ACLs) or encryption\n  Automated deployment\n Allow use of Continuous Delivery (CD) pipelines Incorporate automated testing principals, such as Canary deployments    Federation of service\n  Tools  Git - version control GnuPG - Encryption key management  This can be replaced with a corporate Key Management Service (KMS) if your organisation supports this type of service.   Secrets OPerationS (SOPS)  Encryption of secrets to allow configuration to be securely placed in version control. SOPS allows full file encryption much like many other tools, however, individual values within certain files can be selectively encrypted. This allows the majority of the file that does not pose a site specific security risk to be available for review and sharing amongst Federated support teams. This should also comply with most security team requirements (please ensure this is the case) Can utilise GnuPG keys for encryption but also has the ability to incorporate more Corporate type Key Management Services (KMS) and role based groups (such as AWS AIM accounts)   git-secrets  Git enhancement that utilises pattern matching to help prevent sensitive information being submitted to version control by accident. Warning Does not replace diligence but can help safe guard against mistakes.      ","categories":"","description":"","excerpt":"Requirements and rationals   Collaboration and knowledge share\nTool …","ref":"/docs/charts/operations/recommendations/","tags":"","title":"Operational recommendations"},{"body":"References (Must reads!)  The Chart Best Practices Guide Best Practices for Creating Production-Ready Helm charts Open Source Initiative licenses  ","categories":"","description":"","excerpt":"References (Must reads!)  The Chart Best Practices Guide Best …","ref":"/docs/charts/development/references/","tags":"","title":"References"},{"body":"Hugo shortcodes Also see:\n Hugo documentation on shortcodes Docsy documentation on shortcodes  Custom shortcodes absref An extension of the Hugo shortcode ref to instead return an absolute URL based on the baseURL in config.toml.\nThe path is relative to the virtual content directory mounted from all repositories.  {{\u003cabsref\"docs/charts\"\u003e}} renders to:\nhttps://australian-imaging-service.github.io/docs/charts/ absrelref An extension of the Hugo shortcode relref to instead return an absolute URL based on the baseURL in config.toml.\nThe path is relative to the current file in the virtual content tree mounted from all repositories.  {{\u003cabsrelref\"../../charts\"\u003e}} renders to:\nhttps://australian-imaging-service.github.io/docs/charts/ code Renders a code block with syntax highlighting and a header containing a file name.\n{{\u003ccodeyaml\"filename.yaml\"\u003e}} key1: value1 foo: bar metadata: name: lipsum {{\u003c/code\u003e}}  filename.yaml key1:value1foo:barmetadata:name:lipsum  md Renders the markdown in the referenced filepath.\nThe path is relative to the virtual content directory mounted from all repositories.  {{\u003cmd\"docs/_index.md\"\u003e}} render Renders the contents in the referenced filepath with one of Hugo’s supported content formats.\nThe path is relative to the virtual content directory mounted from all repositories.  {{\u003crender\"path/to/file.rst\"\"rst\"\u003e}} rst Renders the Restructured Text in the referenced filepath.\nThe path is relative to the virtual content directory mounted from all repositories.  {{\u003crst\"path/to/file.rst\"\u003e}} Docsy shortcodes This is only a subset of the shortcodes available from Docsy, see the Docsy documentation for more details.\nalert Alert title Alert contents  {{%alerttitle=\"Alert title\"color=\"warning\"%}} Alert contents {{%/alert%}} pageinfo Page info content\n {{%pageinfocolor=\"primary\"%}} Page info content {{%/pageinfo%}} ","categories":"","description":"","excerpt":"Hugo shortcodes Also see:\n Hugo documentation on shortcodes Docsy …","ref":"/docs/contributing/documentation/shortcodes/","tags":"","title":"Shortcodes"},{"body":"Metadata    Instrument Source Logic Config Source XNAT Destination     Additional Info field: Project:HT-003 (0010,4000) Patient’s Comments (0010,4000) Patient’s Comments Project: HT-003   Lastname:Mukherjee\nFirstname:Sheep1\nTitle: (0010,0010) Patient’s Name (0010,0010) Patient’s Name Subject: Mukherjee_sheep1   Patient ID: Patient01 (0010,0020) Patient ID (0010,0020) Patient ID +\n(0032,0012) Study ID Issuer Session: Patient01_210301-15_47_19-DST-IVS    (0020,0010) Study UID (0020,0010) Study UID SCAN ID: 990     10.25910/5cf9f821b4c94 [IID] TBD      [QCPID] TBD    [Path] needs to be determined. It can be on instrument, or the export location on RDS.\nData    Original structure Note XNAT structure     [Path]//[File].jpg To be covered with DICOM2JPEG Resource file   [Path]//[File].avi  Resource file   [Path]//[File].txt  Resource file   [Path]//[File].csv  Resource file    ","categories":"","description":"","excerpt":"Metadata    Instrument Source Logic Config Source XNAT Destination …","ref":"/docs/instruments/siemens-artis-pheno/","tags":"","title":"Siemens ARTIS pheno"},{"body":"Development workstation with Multipass on Windows 10 Requirements:\n An enabled Hypervisor, either Hyper-V (recommended) or VirtualBox (introduces certain networking issues, if you are using VirtualBox on Windows 10 then use the VirtualBox UI directly or another package such as Vagrant)  Install Hyper-V on Windows 10 Oracle VirtualBox install on Windows hosts  Oracle VirtualBox Downloads     Administrative access to Windows 10 workstation. This is required for:  Enabling Hyper-V if not already configured, or installing Oracle VirtualBox Installing Multipass Altering the local DNS override file c:\\Windows\\System32\\drivers\\etc\\hosts    Windows PowerShell console as Administrator Right click Windows PowerShell and select Run as Administrator, enter your Admin credentials. From the Administrator: Windows PowerShell console perform the following.\n Open the DNS hosts file for editing.  Warning Edit this file with care and ensure that you only append entries while leaving the original entries intact.\nAlso be aware that you have started Notepad as an Administrator allowing this application to be able to edit any file on your system. Close the editor and PowerShell console if you intend to leave your workstation!\n PS C:\\\u003e notepad.exe C:\\Windows\\System32\\drivers\\etc\\hosts  Verify Hyper-V state; the bellow shows that Hyper-V is Enabled on this workstation  PS C:\\\u003e Get-WindowsOptionalFeature -FeatureName Microsoft-Hyper-V-All -Online FeatureName : Microsoft-Hyper-V-All DisplayName : Hyper-V Description : Provides services and management tools for creating and running virtual machines and their resources. RestartRequired : Possible State : Enabled CustomProperties : If this is not the case!\nPS C:\\\u003e Enable-WindowsOptionalFeature -Online -FeatureName Microsoft-Hyper-V -All Download, install and setup Multipass From the Multipass website, verify that your Windows 10 workstation meets the minimum requirements and then download the Windows installation file.\n Select Start button and then select Settings. In Settings, select System \u003e About or type about in the search box. Under Windows specifications verify Edition and Version  Follow the installation instructions from the Multipass site selecting the preferred Hypervisor.\nNB: The Environment variable that configure the search PATH to find the Multipass binaries will not be available until you logout and log back in.\nEdit the workstations local DNS lookup/override file This is required to direct your workstations browser and other clients to the development VM which runs your CTP and/or XNAT service.\nFor each service requiring a DNS entry you will need to add an entry into your hosts file. From your Notepad application opened as an Administrator you will need to enter the following.\n C:\\Windows\\System32\\drivers\\etc\\hosts IP_Address_of_the_VM\tfqdn.service.name fqdn2.service.name  Get the IP address of your VM\nPS C:\\\u003e multipass exec vm-name -- ip addr So if your VM’s IP address is 192.168.11.93 and your service FQDN is xnat.cmca.dev.local add the following entry into C:\\Windows\\System32\\drivers\\etc\\hosts file and save.\n C:\\Windows\\System32\\drivers\\etc\\hosts 192.168.11.93\txnat.cmca.dev.local  Launch Ubuntu 20.04 LTS (Focal) with AIS development tools NB: This may take some time  PS C:\\Users\\00078081\\ais\u003e Invoke-WebRequest https://raw.githubusercontent.com/Australian-Imaging-Service/charts/main/contrib/cloud-init/user-data-dev-microk8s.yaml -OutFile user-data-dev-microk8s.yaml PS C:\\Users\\00078081\\ais\u003e multipass launch --cpus 4 --mem 2G -nais-dev --cloud-init .\\user-data-dev-microk8s.yaml ","categories":"","description":"","excerpt":"Development workstation with Multipass on Windows 10 Requirements:\n An …","ref":"/docs/charts/development/windows10-multipass-k8s/","tags":"","title":"Windows 10+Multipass"},{"body":"# add the required helm repositories helm repo add bitnami https://charts.bitnami.com/bitnami # import the helm chart dependencies (e.g., PostgreSQL) from the xnat chart directory # ensure you have cloned the repo and changed to charts/xnat directory before running this command helm dependency update # view the helm output without deployment from the xnat chart directory helm install --debug --dry-run xnat ais/xnat 2\u003e\u00261 |less # create xnat namespace in kubernetes kubectl create ns xnat # Deploy the AIS XNAT service helm upgrade xnat ais/xnat --install --values ./my-site-overrides.yaml --namespace xnat # Watch the AIS goodness watch kubectl -nxnat get all # watch the logs scroll by kubectl -nxnat logs xnat-xnat-web-0 -f # find out what happened if pod does not start kubectl -nxnat get pod xnat-xnat-web-0 -o json # view the persistent volumes kubectl -nxnat get pvc,pv # view the content of a secret kubectl -nxnat get secret xnat-xnat-web -o go-template='{{ index .data \"xnat-conf.properties\" }}' | base64 -d # tear it all down helm delete xnat -nxnat kubectl -nxnat delete pod,svc,pvc --all kubectl delete namespace xnat ","categories":"","description":"","excerpt":"# add the required helm repositories helm repo add bitnami …","ref":"/docs/charts/development/xnat-chart-readme/","tags":"","title":"XNAT chart README"},{"body":"Background Due to the number of systems and audiences involved, the current documentation plan is for the canonical documentation to be within the Github repository, using GitHub pages.\nThis provides a number of easy to use tools and systems to help keep standardised, versions and readable documentation.\nStrategy  Main AIS documentation to be at the top level https://australian-imaging-service.github.io/ Each repository will self document Using Hugo modules the documentation for each repository will be integrated into the main documentation, which link out to the repositories as necessary - see Integrating a new repository for more details.  Documentation from each repository is effectively mounted in a virtual file tree where the documentation is then generated from. See the hugo docs for more info.    Documentation  Should be written in Markdown Should be in a directory called docs in the top level of each repository. We have chosen this methodology as it allows for versioned documentation that matches the code base. Either add themselves to the main site or ask an administrator to add them.  Integrating a new repository    ","categories":"","description":"","excerpt":"Background Due to the number of systems and audiences involved, the …","ref":"/docs/","tags":"","title":"Documentation"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/contributing/","tags":"","title":"Contributing"},{"body":" Placeholder\n ","categories":"","description":"Page description for heading and indexes.","excerpt":"Page description for heading and indexes.","ref":"/docs/facility-guides/connect-machines/","tags":"","title":"Connect machines"},{"body":" Placeholder\n ","categories":"","description":"Page description for heading and indexes.","excerpt":"Page description for heading and indexes.","ref":"/docs/facility-guides/ctp/","tags":"","title":"CTP"},{"body":" Placeholder\n ","categories":"","description":"Page description for heading and indexes.","excerpt":"Page description for heading and indexes.","ref":"/docs/getting-started/upload-a-dicom/","tags":"","title":"Upload a DICOM file"},{"body":" Placeholder\n ","categories":"","description":"Page description for heading and indexes.","excerpt":"Page description for heading and indexes.","ref":"/docs/facility-guides/xnat/","tags":"","title":"XNat"},{"body":"List of steps to be followed to deploy XNAT in Linode LKE using Helm charts\n1.LKE Cluster Setup:\nSet up the Linode LKE cluster using the link https://www.linode.com/docs/guides/how-to-deploy-an-lke-cluster-using-terraform/ (Please note that a separate documentation for setting up LKE Cluster using Terraform will be coming up soon)\n2.Preparing for Tweaks pertaining to Linode:\nAs we are tweaking XNAT Values related to PV access modes, let us check out the charts repo rather than using the adding AIS helm chart repository.\n git clone https://github.com/Australian-Imaging-Service/charts.git  3.Actual Tweaks:\nReplace the access modes of all Volumes from “ReadWriteMany” to “ReadWriteOnce” in charts/releases/xnat/charts/xnat-web This is because Linode storage only supports “ReadWriteOnce” at this point of time.\n4.Dependency Update:\nUpdate the dependency by switching to charts/releases/xnat and execute the following\n helm dependency update  5.XNAT Initial Installation:\nGo to charts/releases and install xnat using helm.\n kubectl create namespace xnat helm install xnat-deployment xnat --values YOUR-VALUES-FILE --namespace=xnat  The XNAT \u0026 POSTGRES service should be up \u0026 running fine. Linode Storage Class “linode-block-storage-retain” should have automatically come in place \u0026 PVs will be auto created to be consumed by our mentioned PVCs.\n6.Ingress Controller/Load balancer Installation:\nInstall Ingress Controller and provision a Load balancer (Nodebalancer in Linode) by executing these commands\n helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx helm repo update helm install ingress-nginx ingress-nginx/ingress-nginx  You may see an output like below\n NAME: ingress-nginx\nLAST DEPLOYED: Mon Aug 2 11:51:32 2021\nNAMESPACE: default\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\nNOTES:\nThe ingress-nginx controller has been installed.\nIt may take a few minutes for the LoadBalancer IP to be available.\n 7.Domain Mapping:\nGet the External IP address of the Loadbalancer by running the below command and assign it to any domain or subdomain.\n Example: cloud.neura.edu.au is the subdomain for which the loadbalancer IP is assigned in my case. Please replace it with your domain in this and all upcoming steps\n  kubectl --namespace default get services -o wide -w ingress-nginx-controller  8.HTTP Traffic Routing via Ingress:\nIt is time to create a Ingress object that directs the traffic based on the host/domain to the already available XNAT service. Get the XNAT service name by issuing the below command and choose the service name that says TYPE as ClusterIP\n kubectl get svc -nxnat -l \"app.kubernetes.io/name=xnat-web\"  Example: xnat-deployment-xnat-web\nUsing the above service name, write an ingress object to route the external traffic based on the domain name.\n apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: xnat-ingress namespace: xnat annotations: kubernetes.io/ingress.class: nginx spec: rules: - host: cloud.neura.edu.au http: paths: - pathType: Prefix path: \"/\" backend: service: name: xnat-deployment-xnat-web port: number: 80  9.Delete the HTTP Ingress project:\nAfter the creation of this Ingress object, make sure cloud.neura.edu.au is routed to the XNAT application over HTTP successfully.Let us delete the ingress object after checking because we will be creating another one with TLS to use HTTPS.\n kubectl delete ingress xnat-ingress -nxnat  10.Install cert-manager for Secure Connection HTTPS\nInstall cert-manager’s CRDs.\n kubectl apply -f https://github.com/jetstack/cert-manager/releases/download/v1.3.1/cert-manager.crds.yaml  Create a cert-manager namespace.\n kubectl create namespace cert-manager  Add the Helm repository which contains the cert-manager Helm chart.\n helm repo add jetstack https://charts.jetstack.io  Update your Helm repositories.\n helm repo update  Install the cert-manager Helm chart.\n helm install \\ cert-manager jetstack/cert-manager \\ --namespace cert-manager \\ --version v1.3.1  Verify that the corresponding cert-manager pods are now running.\n kubectl get pods --namespace cert-manager  You should see a similar output:\n NAME READY STATUS RESTARTS AGE\ncert-manager-579d48dff8-84nw9 1/1 Running 3 1m\ncert-manager-cainjector-789955d9b7-jfskr 1/1 Running 3 1m\ncert-manager-webhook-64869c4997-hnx6n 1/1 Running 0 1m\n 11.Creation of ClusterIssuer to Issue certificates:\nCreate a manifest file named acme-issuer-prod.yaml that will be used to create a ClusterIssuer resource on your cluster. Ensure you replace user@example.com with your own email address.\n apiVersion: cert-manager.io/v1 kind: ClusterIssuer metadata: name: letsencrypt-prod namespace: xnat spec: acme: email: user@example.com server: https://acme-v02.api.letsencrypt.org/directory privateKeySecretRef: name: letsencrypt-secret-prod solvers: - http01: ingress: class: nginx  12.HTTPS Routing with Ingress object leveraging ClusterIssuer:\nProvision a new Ingress object to use the clusterIssuer for the generation of the certificate and use it\n apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: xnat-ingress-https namespace: xnat annotations: kubernetes.io/ingress.class: \"nginx\" cert-manager.io/cluster-issuer: \"letsencrypt-prod\" spec: tls: - hosts: - cloud.neura.edu.au secretName: xnat-tls rules: - host: cloud.neura.edu.au http: paths: - pathType: Prefix path: \"/\" backend: service: name: xnat-deployment-xnat-web port: number: 80  After the creation of the above ingress https://cloud.neura.edu.au/ should bring up the XNAT application in the web browser\nReference Links\nLKE set up using Cloud Manager - https://www.linode.com/docs/guides/deploy-and-manage-a-cluster-with-linode-kubernetes-engine-a-tutorial/\nLKE set up using Terraform - https://www.linode.com/docs/guides/how-to-deploy-an-lke-cluster-using-terraform/\nLinode Storage Class - https://www.linode.com/docs/guides/deploy-volumes-with-the-linode-block-storage-csi-driver/\nIngress Controller \u0026 Loadbalancer- https://www.linode.com/docs/guides/how-to-deploy-nginx-ingress-on-linode-kubernetes-engine/\nHTTP to HTTPS using cert-manager - https://www.linode.com/docs/guides/how-to-configure-load-balancing-with-tls-encryption-on-a-kubernetes-cluster\n","categories":"","description":"","excerpt":"List of steps to be followed to deploy XNAT in Linode LKE using Helm …","ref":"/docs/charts/deployment/linode-setup/","tags":"","title":""},{"body":"Deployments of AIS released service The /docs/Deployment folder is a dump directory for any documentation related to deployment of the AIS released services. This includes, but is not limited to, deployment examples:\n from different AIS sites utilising alternate Cloud services or on-prem deployments configuration snippets  Jekyll is used to render these documents and any MarkDown files with the appropriate FrontMatter tags will appear in the Deployment drop-down menu item.\nhttps://australian-imaging-service.github.io/charts/\n","categories":"","description":"","excerpt":"Deployments of AIS released service The /docs/Deployment folder is a …","ref":"/docs/charts/deployment/readme/","tags":"","title":""},{"body":"Development instructions, recommendations, etc… The /docs/_development folder is a dump directory for any documentation related to setup and practices of development related to the AIS released services.\nJekyll is used to render these documents and any MarkDown files with the appropriate FrontMatter tags will appear in the Development drop-down menu item.\nhttps://australian-imaging-service.github.io/charts/\n","categories":"","description":"","excerpt":"Development instructions, recommendations, etc… The /docs/_development …","ref":"/docs/charts/development/readme/","tags":"","title":""},{"body":"Operational recommendations The /docs/_operational folder is a dump directory for any documentation related to the day-to-day runnings of AIS released services. This includes, but is not limited to, operational tasks such as:\n Administration tasks Automation Release management Backup and disaster recovery  Jekyll is used to render these documents and any MarkDown files with the appropriate FrontMatter tags will appear in the Operational drop-down menu item.\nhttps://australian-imaging-service.github.io/charts/\n","categories":"","description":"","excerpt":"Operational recommendations The /docs/_operational folder is a dump …","ref":"/docs/charts/operations/readme/","tags":"","title":""},{"body":"Contributing to the AIS Charts project source The /docs/_contributing folder is a dump directory for any documentation related to contribute work and/or amendments to this project.\nJekyll is used to render these documents and any MarkDown files with the appropriate FrontMatter tags will appear in the Contributing drop-down menu item.\nhttps://australian-imaging-service.github.io/charts/\n","categories":"","description":"","excerpt":"Contributing to the AIS Charts project source The /docs/_contributing …","ref":"/docs/contributing/readme/","tags":"","title":""},{"body":"asdf\n","categories":"","description":"","excerpt":"asdf\n","ref":"/logos/","tags":"","title":""},{"body":"  #td-cover-block-0 { background-image: url(/featured-background_hu376e1fbab6ce6c455a2b3aa5c258c0d9_496231_960x540_fill_q75_catmullrom_top.jpg); } @media only screen and (min-width: 1200px) { #td-cover-block-0 { background-image: url(/featured-background_hu376e1fbab6ce6c455a2b3aa5c258c0d9_496231_1920x1080_fill_q75_catmullrom_top.jpg); } }  Australian Imaging Service Learn More   Download            As part of the ARDC 2019 Project’s scheme, we are building a distributed, national federation for securely managing and analyzing imaging data for research, with a focus on radiology and biomedical applications.\n      New chair metrics!  The Goldydocs UI now shows chair size metrics by default.\nPlease follow this space for updates!\n   Contributions welcome!  We do a Pull Request contributions workflow on GitHub. New users are always welcome!\nRead more …\n   Follow us on Twitter!  For announcement of latest features etc.\nRead more …\n    --  Project     Universities and clinical sites across Australia are struggling to manage large volumes of imaging data, while balancing patient privacy and the need for sharing and accessibility in the research community. The Australian Imaging Service will transform the imaging and radiology sector by leveraging institutional investments and providing enhanced data management and analysis. The distributed federation will consist of multiple institutional deployments linked with a federated search layer, common community practice, support for expanded data types and a Trusted Tool Repository ensuring ongoing ownership and accountability of data. Members  University of Sydney, Australian Research Data Commons, National Imaging Facilities, Australian Catholic University Macquarie University, Monash University, Queensland University of Technology, University of New South Wales, University of Queensland, University of Swinburne, University of Western Australia, University of Wollongong, The Florey, South Australian Health and Medical Research Institute (SAHMRI)  Documentation and Support  Documentation overview AIS Repositories User Guide  Supporting Technologies  XNAT, OHIF, REDCap, CLARA, Gadgetron,       This is another Section     -- ","categories":"","description":"","excerpt":"  #td-cover-block-0 { background-image: …","ref":"/","tags":"","title":"Australian Imaging Service"},{"body":" Australian Imaging Service Learn More   Download            As part of the ARDC 2019 Project’s scheme, we are building a distributed, national federation for securely managing and analyzing imaging data for research, with a focus on radiology and biomedical applications.\n      New chair metrics!  The Goldydocs UI now shows chair size metrics by default.\nPlease follow this space for updates!\n   Contributions welcome!  We do a Pull Request contributions workflow on GitHub. New users are always welcome!\nRead more …\n   Follow us on Twitter!  For announcement of latest features etc.\nRead more …\n    --  Project     Universities and clinical sites across Australia are struggling to manage large volumes of imaging data, while balancing patient privacy and the need for sharing and accessibility in the research community. The Australian Imaging Service will transform the imaging and radiology sector by leveraging institutional investments and providing enhanced data management and analysis. The distributed federation will consist of multiple institutional deployments linked with a federated search layer, common community practice, support for expanded data types and a Trusted Tool Repository ensuring ongoing ownership and accountability of data. Members      --  Monash University --          University of Sydney, Australian Research Data Commons, National Imaging Facilities, Australian Catholic University Macquarie University, Monash University, Queensland University of Technology, University of New South Wales, University of Queensland, University of Swinburne, University of Western Australia, University of Wollongong, The Florey, South Australian Health and Medical Research Institute (SAHMRI)  -- Documentation and Support  Documentation overview AIS Repositories User Guide  Supporting Technologies  XNAT, OHIF, REDCap, CLARA, Gadgetron,       This is another Section     -- ","categories":"","description":"","excerpt":" Australian Imaging Service Learn More   Download            As part …","ref":"/_staging/","tags":"","title":"Australian Imaging Service"},{"body":"","categories":"","description":"","excerpt":"","ref":"/search/","tags":"","title":"Search Results"}]